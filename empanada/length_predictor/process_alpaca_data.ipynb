{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output length predictor training\n",
    "\n",
    "Alpaca data source: https://huggingface.co/datasets/yahma/alpaca-cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/holystore01/LABS/mzitnik_lab/Lab/yeh803/cache/miniforge3/envs/preble/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "\n",
    "alpaca = load_dataset(\"yahma/alpaca-cleaned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence embeddings:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 384])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentences = [\"test how are you\", \"how test\", \"do you know test\"]  # FIXME\n",
    "\n",
    "# # Load model from HuggingFace Hub\n",
    "# tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "# bert_model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2', device_map=\"cpu\")\n",
    "\n",
    "# # Tokenize sentences\n",
    "# encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# # Compute token embeddings\n",
    "# with torch.no_grad():\n",
    "#     model_output = bert_model(**encoded_input)\n",
    "\n",
    "# # Perform pooling\n",
    "# sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "# # Normalize embeddings\n",
    "# sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "\n",
    "# print(\"Sentence embeddings:\")\n",
    "# # print(sentence_embeddings)\n",
    "# sentence_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>output</th>\n",
       "      <th>input</th>\n",
       "      <th>instruction</th>\n",
       "      <th>output_token_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1. Eat a balanced and nutritious diet: Make su...</td>\n",
       "      <td></td>\n",
       "      <td>Give three tips for staying healthy.</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The three primary colors are red, blue, and ye...</td>\n",
       "      <td></td>\n",
       "      <td>What are the three primary colors?</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>An atom is the basic building block of all mat...</td>\n",
       "      <td></td>\n",
       "      <td>Describe the structure of an atom.</td>\n",
       "      <td>246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>There are several ways to reduce air pollution...</td>\n",
       "      <td></td>\n",
       "      <td>How can we reduce air pollution?</td>\n",
       "      <td>275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I had to make a difficult decision when I was ...</td>\n",
       "      <td></td>\n",
       "      <td>Pretend you are a project manager of a constru...</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51755</th>\n",
       "      <td>Yes</td>\n",
       "      <td>Text: John went out for a walk with his dog Ro...</td>\n",
       "      <td>You will be given a piece of text about an eve...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51756</th>\n",
       "      <td>True</td>\n",
       "      <td>Text: Michael Jordan is an American former pro...</td>\n",
       "      <td>You will be given a paragraph of text with var...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51757</th>\n",
       "      <td>True</td>\n",
       "      <td>Text: A tree fell over in the wind and caused ...</td>\n",
       "      <td>You will be given a piece of text about an eve...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51758</th>\n",
       "      <td>Backwards</td>\n",
       "      <td>Steps: ['She takes out her books', 'The teache...</td>\n",
       "      <td>I will give you a list of steps.  You need to ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51759</th>\n",
       "      <td>The statement made in the text is a fact.</td>\n",
       "      <td>Text: The sky was very cloudy today.</td>\n",
       "      <td>Given a piece of text, you need to output whet...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51760 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  output  \\\n",
       "0      1. Eat a balanced and nutritious diet: Make su...   \n",
       "1      The three primary colors are red, blue, and ye...   \n",
       "2      An atom is the basic building block of all mat...   \n",
       "3      There are several ways to reduce air pollution...   \n",
       "4      I had to make a difficult decision when I was ...   \n",
       "...                                                  ...   \n",
       "51755                                                Yes   \n",
       "51756                                               True   \n",
       "51757                                               True   \n",
       "51758                                          Backwards   \n",
       "51759          The statement made in the text is a fact.   \n",
       "\n",
       "                                                   input  \\\n",
       "0                                                          \n",
       "1                                                          \n",
       "2                                                          \n",
       "3                                                          \n",
       "4                                                          \n",
       "...                                                  ...   \n",
       "51755  Text: John went out for a walk with his dog Ro...   \n",
       "51756  Text: Michael Jordan is an American former pro...   \n",
       "51757  Text: A tree fell over in the wind and caused ...   \n",
       "51758  Steps: ['She takes out her books', 'The teache...   \n",
       "51759               Text: The sky was very cloudy today.   \n",
       "\n",
       "                                             instruction  output_token_len  \n",
       "0                   Give three tips for staying healthy.               151  \n",
       "1                     What are the three primary colors?                67  \n",
       "2                     Describe the structure of an atom.               246  \n",
       "3                       How can we reduce air pollution?               275  \n",
       "4      Pretend you are a project manager of a constru...               153  \n",
       "...                                                  ...               ...  \n",
       "51755  You will be given a piece of text about an eve...                 3  \n",
       "51756  You will be given a paragraph of text with var...                 3  \n",
       "51757  You will be given a piece of text about an eve...                 3  \n",
       "51758  I will give you a list of steps.  You need to ...                 3  \n",
       "51759  Given a piece of text, you need to output whet...                12  \n",
       "\n",
       "[51760 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HF native\n",
    "from functools import partial\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] # First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "def get_token_len(sentences, tokenizer):\n",
    "    encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "    return encoded_input[\"attention_mask\"].detach().cpu().sum(dim=1).numpy()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "bert_model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2', device_map=\"cpu\")\n",
    "\n",
    "alpaca_df = list(alpaca.data.values())[0].to_pandas()\n",
    "alpaca_df[\"output_token_len\"] = get_token_len(alpaca_df[\"output\"].values.tolist(), tokenizer)\n",
    "alpaca_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (alpaca_df[\"instruction\"] == \"\").sum() == 0\n",
    "assert (alpaca_df[\"input\"] == \"\").sum() != 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1539363/930882405.py:3: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  row[1] + (\n",
      "/tmp/ipykernel_1539363/930882405.py:5: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  row[1].endswith(\".\") or row[1].endswith(\"?\") or row[1].endswith(\"!\") or\n",
      "/tmp/ipykernel_1539363/930882405.py:14: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  ) if row[0] != \"\" else \"\"\n",
      "/tmp/ipykernel_1539363/930882405.py:6: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  row[1].endswith(\"\\n\") or row[1].endswith(\":\")\n",
      "/tmp/ipykernel_1539363/930882405.py:9: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  (\"\" if row[1].endswith(\" \") else \" \") + \\\n",
      "/tmp/ipykernel_1539363/930882405.py:10: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  row[0] + (\n",
      "/tmp/ipykernel_1539363/930882405.py:11: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"\" if row[1].endswith(\".\") or row[1].endswith(\"?\") or\n",
      "/tmp/ipykernel_1539363/930882405.py:12: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  row[1].endswith(\"!\") or row[1].endswith(\"\\n\") or row[1].endswith(\":\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Give three tips for staying healthy.',\n",
       "       'What are the three primary colors?',\n",
       "       'Describe the structure of an atom.', ...,\n",
       "       \"You will be given a piece of text about an event that has happened. Your job is to determine if the event could have reasonably happened, based on your knowledge and commonsense. If it could have reasonably happened, output 'True', otherwise output 'False'. Text: A tree fell over in the wind and caused damage to my car.\",\n",
       "       \"I will give you a list of steps.  You need to determine if the steps are going forwards or backwards in time by outputting 'Forwards' or 'Backwards'. Steps: ['She takes out her books', 'The teacher hands back the papers', 'She walks into class', 'The bell rings'].\",\n",
       "       \"Given a piece of text, you need to output whether the statements made in the text are opinions or facts. An opinion is defined as a statement that cannot be proven true or false and is usually based on someone's beliefs. A fact is defined as a statement that can be proven true or false and is not based on someone's beliefs. Text: The sky was very cloudy today.\"],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpaca_df[\"input_text\"] = alpaca_df[[\"input\", \"instruction\"]].apply(\n",
    "    lambda row: \n",
    "        row[1] + (\n",
    "            \"\" if \n",
    "            row[1].endswith(\".\") or row[1].endswith(\"?\") or row[1].endswith(\"!\") or\n",
    "            row[1].endswith(\"\\n\") or row[1].endswith(\":\")\n",
    "            else \".\"\n",
    "        ) + (\n",
    "            (\"\" if row[1].endswith(\" \") else \" \") + \\\n",
    "            row[0] + (\n",
    "                \"\" if row[1].endswith(\".\") or row[1].endswith(\"?\") or \n",
    "                row[1].endswith(\"!\") or row[1].endswith(\"\\n\") or row[1].endswith(\":\") \n",
    "                else \".\"\n",
    "            ) if row[0] != \"\" else \"\"\n",
    "        ),\n",
    "    axis=1\n",
    ")\n",
    "alpaca_df[\"input_text\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "alpaca_hf = alpaca_df[[\"input_text\", \"output_token_len\"]].rename(columns={\"input_text\": \"text\"})\n",
    "alpaca_hf[\"output_token_len\"].hist(ec=\"white\", bins=50, grid=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of buckets\n",
    "num_buckets = 20\n",
    "\n",
    "# Calculate the percentiles\n",
    "percentiles = np.linspace(0, 100, num_buckets + 1)\n",
    "\n",
    "# Get the bucket boundaries, with the last bucket's max set to 512\n",
    "bucket_edges = list(np.percentile(alpaca_hf['output_token_len'], percentiles[:-1])) + [512]\n",
    "\n",
    "# Create the bucket labels (max values for each bucket)\n",
    "bucket_labels = np.arange(len(bucket_edges)-1)\n",
    "bucket_max = bucket_edges[1:]\n",
    "\n",
    "# Replace label values with the max value of the bucket\n",
    "alpaca_hf['label'] = pd.cut(alpaca_hf['output_token_len'], bins=bucket_edges, include_lowest=True,\n",
    "                            labels=bucket_labels, \n",
    "                            )\n",
    "alpaca_hf['label'] = alpaca_hf['label'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train_full: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 31056\n",
      "    })\n",
      "    train_small: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 15528\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 10352\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 10352\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split into train, validation, and test sets\n",
    "train_full, test = train_test_split(alpaca_hf.drop(columns=[\"output_token_len\"]), test_size=0.2, random_state=42)\n",
    "train_full, val = train_test_split(train_full, test_size=0.25, random_state=42)\n",
    "\n",
    "# Create two train datasets: one half-sized of the other\n",
    "_, train_small = train_test_split(train_full, test_size=0.5, random_state=42)\n",
    "\n",
    "# Convert DataFrame to Hugging Face Dataset\n",
    "train_full_ds = Dataset.from_pandas(train_full.reset_index(drop=True))\n",
    "train_small_ds = Dataset.from_pandas(train_small.reset_index(drop=True))\n",
    "val_ds = Dataset.from_pandas(val.reset_index(drop=True))\n",
    "test_ds = Dataset.from_pandas(test.reset_index(drop=True))\n",
    "\n",
    "# Combine into a DatasetDict\n",
    "alpaca_ds = DatasetDict({\n",
    "    \"train_full\": train_full_ds,\n",
    "    \"train_small\": train_small_ds,\n",
    "    \"validation\": val_ds,\n",
    "    \"test\": test_ds\n",
    "})\n",
    "\n",
    "# Verify the structure of DatasetDict\n",
    "print(alpaca_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/31056 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31056/31056 [00:05<00:00, 6024.05 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15528/15528 [00:02<00:00, 6843.90 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10352/10352 [00:01<00:00, 6242.09 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10352/10352 [00:01<00:00, 8794.36 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def preprocess_function(ds):\n",
    "    tokens = tokenizer(ds[\"text\"], padding=True, truncation=True, return_tensors='pt')\n",
    "    tokens[\"labels\"] = ds[\"label\"]\n",
    "    return tokens\n",
    "\n",
    "train_full_ds_tokenized = train_full_ds.map(preprocess_function, batched=True)\n",
    "train_small_ds_tokenized = train_small_ds.map(preprocess_function, batched=True)\n",
    "val_ds_tokenized = val_ds.map(preprocess_function, batched=True)\n",
    "test_ds_tokenized = test_ds.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictor\n",
    "actn2actfunc = {'relu': nn.ReLU(inplace=True), 'leakyrelu': nn.LeakyReLU(inplace=True), 'tanh': nn.Tanh(), 'sigmoid': nn.Sigmoid(), 'selu': nn.SELU(inplace=True), 'softplus': nn.Softplus(), 'gelu': nn.GELU(), None: nn.Identity()}\n",
    "\n",
    "class MLPAdaptor(nn.Module):\n",
    "    def __init__(self, in_dim: int, hidden_dims: list, output_dim: int, p: float, norm: str, actn: str, order: str = 'nd'):\n",
    "        super(MLPAdaptor, self).__init__()\n",
    "        self.n_layer = len(hidden_dims) - 1\n",
    "        self.in_dim = in_dim\n",
    "        \n",
    "        try:\n",
    "            actn = actn2actfunc[actn]\n",
    "        except:\n",
    "            print(actn)\n",
    "            raise NotImplementedError\n",
    "\n",
    "        # input layer\n",
    "        layers = [nn.Linear(self.in_dim, hidden_dims[0]), actn]\n",
    "        # hidden layers\n",
    "        for i in range(self.n_layer):\n",
    "            layers += self.compose_layer(\n",
    "                in_dim=hidden_dims[i], out_dim=hidden_dims[i+1], norm=norm, actn=actn, p=p, order=order\n",
    "            )\n",
    "        # output layers\n",
    "        layers.append(nn.Linear(hidden_dims[-1], output_dim))\n",
    "\n",
    "        self.fc = nn.Sequential(*layers)\n",
    "\n",
    "    def compose_layer(\n",
    "        self,\n",
    "        in_dim: int,\n",
    "        out_dim: int,\n",
    "        norm: str,\n",
    "        actn: nn.Module,\n",
    "        p: float = 0.0,\n",
    "        order: str = 'nd'\n",
    "    ):\n",
    "        norm2normlayer = {'bn': nn.BatchNorm1d(in_dim), 'ln': nn.LayerNorm(in_dim), None: None, 'None': None}  # because in_dim is only fixed here\n",
    "        try:\n",
    "            norm = norm2normlayer[norm]\n",
    "        except:\n",
    "            print(norm)\n",
    "            raise NotImplementedError\n",
    "        # norm --> dropout or dropout --> norm\n",
    "        if order == 'nd':\n",
    "            layers = [norm] if norm is not None else []\n",
    "            if p != 0:\n",
    "                layers.append(nn.Dropout(p))\n",
    "        elif order == 'dn':\n",
    "            layers = [nn.Dropout(p)] if p != 0 else []\n",
    "            if norm is not None:\n",
    "                layers.append(norm)\n",
    "        else:\n",
    "            print(order)\n",
    "            raise NotImplementedError\n",
    "\n",
    "        layers.append(nn.Linear(in_dim, out_dim))\n",
    "        if actn is not None:\n",
    "            layers.append(actn)\n",
    "        return layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.fc(x)\n",
    "        return output\n",
    "    \n",
    "    \n",
    "class OutputLengthPredictor(nn.Module):\n",
    "    def __init__(self, base_model: nn.Module = bert_model, normalize_embeddings: bool = True, adaptor_output_dim: int = num_buckets, adaptor_hidden_dims: list = [256, 128], adaptor_dropout: float = 0.2, adaptor_norm: str = \"ln\", adaptor_actn: str = \"relu\", adaptor_order: str = \"nd\"):\n",
    "        super(OutputLengthPredictor, self).__init__()\n",
    "        self.base_model = base_model\n",
    "        self.normalize_embeddings = normalize_embeddings\n",
    "        self.predictor = MLPAdaptor(in_dim=self.base_model.pooler.dense.out_features, hidden_dims=adaptor_hidden_dims, output_dim=adaptor_output_dim, p=adaptor_dropout, norm=adaptor_norm, actn=adaptor_actn, order=adaptor_order) # NOTE: Currenlt inpur dimension is hard-coded (there MUST BE a pooler layer that contains a dense layer)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    def forward(self, input_ids=None, token_type_ids=None, attention_mask=None, labels=None):\n",
    "        x = self.base_model(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\n",
    "        x = mean_pooling(x, attention_mask)\n",
    "        if self.normalize_embeddings:\n",
    "            x = F.normalize(x, p=2, dim=1)\n",
    "        x = self.predictor(x)\n",
    "        logits = F.softmax(x, dim=1)\n",
    "        \n",
    "        assert labels is not None\n",
    "        loss = self.loss_fn(x, labels.long())\n",
    "        \n",
    "        return {\"loss\": loss, \"logits\": logits}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    roc_auc_score,\n",
    "    matthews_corrcoef,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score\n",
    ")\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=1)  # Get predicted class indices\n",
    "    \n",
    "    # Metrics\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions, average=\"weighted\")\n",
    "    precision = precision_score(labels, predictions, average=\"weighted\")\n",
    "    recall = recall_score(labels, predictions, average=\"weighted\")\n",
    "    try:\n",
    "        auroc = roc_auc_score(labels, logits, multi_class=\"ovr\")\n",
    "    except ValueError:\n",
    "        auroc = float('nan')  # Handle cases where AUROC can't be computed\n",
    "    mcc = matthews_corrcoef(labels, predictions)\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"auroc\": auroc,\n",
    "        \"mcc\": mcc,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/holystore01/LABS/mzitnik_lab/Lab/yeh803/cache/miniforge3/envs/preble/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_1539363/772653080.py:34: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/n/holystore01/LABS/mzitnik_lab/Lab/yeh803/cache/miniforge3/envs/preble/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='600' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [600/600 13:17, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Auroc</th>\n",
       "      <th>Mcc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.979600</td>\n",
       "      <td>2.954008</td>\n",
       "      <td>0.095117</td>\n",
       "      <td>0.060292</td>\n",
       "      <td>0.081166</td>\n",
       "      <td>0.095117</td>\n",
       "      <td>0.590454</td>\n",
       "      <td>0.047503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.790300</td>\n",
       "      <td>2.741416</td>\n",
       "      <td>0.129102</td>\n",
       "      <td>0.071437</td>\n",
       "      <td>0.090394</td>\n",
       "      <td>0.129102</td>\n",
       "      <td>0.737095</td>\n",
       "      <td>0.089894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.592500</td>\n",
       "      <td>2.556885</td>\n",
       "      <td>0.167285</td>\n",
       "      <td>0.116814</td>\n",
       "      <td>0.148160</td>\n",
       "      <td>0.167285</td>\n",
       "      <td>0.799766</td>\n",
       "      <td>0.127400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.441900</td>\n",
       "      <td>2.435339</td>\n",
       "      <td>0.179980</td>\n",
       "      <td>0.135067</td>\n",
       "      <td>0.156151</td>\n",
       "      <td>0.179980</td>\n",
       "      <td>0.821348</td>\n",
       "      <td>0.140064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.338500</td>\n",
       "      <td>2.350745</td>\n",
       "      <td>0.200391</td>\n",
       "      <td>0.156463</td>\n",
       "      <td>0.170584</td>\n",
       "      <td>0.200391</td>\n",
       "      <td>0.832717</td>\n",
       "      <td>0.160405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.269400</td>\n",
       "      <td>2.307064</td>\n",
       "      <td>0.205371</td>\n",
       "      <td>0.171558</td>\n",
       "      <td>0.173142</td>\n",
       "      <td>0.205371</td>\n",
       "      <td>0.835970</td>\n",
       "      <td>0.164674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.195700</td>\n",
       "      <td>2.262937</td>\n",
       "      <td>0.210547</td>\n",
       "      <td>0.186222</td>\n",
       "      <td>0.186364</td>\n",
       "      <td>0.210547</td>\n",
       "      <td>0.841595</td>\n",
       "      <td>0.169914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.121700</td>\n",
       "      <td>2.237548</td>\n",
       "      <td>0.220410</td>\n",
       "      <td>0.185981</td>\n",
       "      <td>0.198451</td>\n",
       "      <td>0.220410</td>\n",
       "      <td>0.843412</td>\n",
       "      <td>0.180852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.072500</td>\n",
       "      <td>2.212836</td>\n",
       "      <td>0.222852</td>\n",
       "      <td>0.197204</td>\n",
       "      <td>0.201080</td>\n",
       "      <td>0.222852</td>\n",
       "      <td>0.846254</td>\n",
       "      <td>0.182951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.027400</td>\n",
       "      <td>2.195557</td>\n",
       "      <td>0.227832</td>\n",
       "      <td>0.202598</td>\n",
       "      <td>0.211800</td>\n",
       "      <td>0.227832</td>\n",
       "      <td>0.847822</td>\n",
       "      <td>0.188339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.966700</td>\n",
       "      <td>2.195223</td>\n",
       "      <td>0.225098</td>\n",
       "      <td>0.202242</td>\n",
       "      <td>0.207244</td>\n",
       "      <td>0.225098</td>\n",
       "      <td>0.846910</td>\n",
       "      <td>0.185074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.939800</td>\n",
       "      <td>2.187499</td>\n",
       "      <td>0.227344</td>\n",
       "      <td>0.208533</td>\n",
       "      <td>0.212025</td>\n",
       "      <td>0.227344</td>\n",
       "      <td>0.847771</td>\n",
       "      <td>0.187082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.911300</td>\n",
       "      <td>2.178777</td>\n",
       "      <td>0.232617</td>\n",
       "      <td>0.214339</td>\n",
       "      <td>0.220481</td>\n",
       "      <td>0.232617</td>\n",
       "      <td>0.848977</td>\n",
       "      <td>0.192781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.889800</td>\n",
       "      <td>2.173913</td>\n",
       "      <td>0.235352</td>\n",
       "      <td>0.216409</td>\n",
       "      <td>0.225134</td>\n",
       "      <td>0.235352</td>\n",
       "      <td>0.849690</td>\n",
       "      <td>0.195773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.870500</td>\n",
       "      <td>2.177615</td>\n",
       "      <td>0.234375</td>\n",
       "      <td>0.216631</td>\n",
       "      <td>0.224597</td>\n",
       "      <td>0.234375</td>\n",
       "      <td>0.848927</td>\n",
       "      <td>0.194616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.839000</td>\n",
       "      <td>2.178649</td>\n",
       "      <td>0.236914</td>\n",
       "      <td>0.220895</td>\n",
       "      <td>0.225873</td>\n",
       "      <td>0.236914</td>\n",
       "      <td>0.848864</td>\n",
       "      <td>0.197152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.835300</td>\n",
       "      <td>2.174767</td>\n",
       "      <td>0.237012</td>\n",
       "      <td>0.222400</td>\n",
       "      <td>0.226645</td>\n",
       "      <td>0.237012</td>\n",
       "      <td>0.849445</td>\n",
       "      <td>0.197185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.823500</td>\n",
       "      <td>2.175029</td>\n",
       "      <td>0.236523</td>\n",
       "      <td>0.221306</td>\n",
       "      <td>0.226479</td>\n",
       "      <td>0.236523</td>\n",
       "      <td>0.849514</td>\n",
       "      <td>0.196788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.809400</td>\n",
       "      <td>2.174417</td>\n",
       "      <td>0.239355</td>\n",
       "      <td>0.223958</td>\n",
       "      <td>0.229027</td>\n",
       "      <td>0.239355</td>\n",
       "      <td>0.849651</td>\n",
       "      <td>0.199763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.792800</td>\n",
       "      <td>2.176104</td>\n",
       "      <td>0.237109</td>\n",
       "      <td>0.222021</td>\n",
       "      <td>0.226643</td>\n",
       "      <td>0.237109</td>\n",
       "      <td>0.849393</td>\n",
       "      <td>0.197404</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/holystore01/LABS/mzitnik_lab/Lab/yeh803/cache/miniforge3/envs/preble/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/n/holystore01/LABS/mzitnik_lab/Lab/yeh803/cache/miniforge3/envs/preble/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/n/holystore01/LABS/mzitnik_lab/Lab/yeh803/cache/miniforge3/envs/preble/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/n/holystore01/LABS/mzitnik_lab/Lab/yeh803/cache/miniforge3/envs/preble/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/n/holystore01/LABS/mzitnik_lab/Lab/yeh803/cache/miniforge3/envs/preble/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/n/holystore01/LABS/mzitnik_lab/Lab/yeh803/cache/miniforge3/envs/preble/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/n/holystore01/LABS/mzitnik_lab/Lab/yeh803/cache/miniforge3/envs/preble/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/n/holystore01/LABS/mzitnik_lab/Lab/yeh803/cache/miniforge3/envs/preble/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/n/holystore01/LABS/mzitnik_lab/Lab/yeh803/cache/miniforge3/envs/preble/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/n/holystore01/LABS/mzitnik_lab/Lab/yeh803/cache/miniforge3/envs/preble/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/n/holystore01/LABS/mzitnik_lab/Lab/yeh803/cache/miniforge3/envs/preble/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/n/holystore01/LABS/mzitnik_lab/Lab/yeh803/cache/miniforge3/envs/preble/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/n/holystore01/LABS/mzitnik_lab/Lab/yeh803/cache/miniforge3/envs/preble/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/n/holystore01/LABS/mzitnik_lab/Lab/yeh803/cache/miniforge3/envs/preble/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/n/holystore01/LABS/mzitnik_lab/Lab/yeh803/cache/miniforge3/envs/preble/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/n/holystore01/LABS/mzitnik_lab/Lab/yeh803/cache/miniforge3/envs/preble/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/n/holystore01/LABS/mzitnik_lab/Lab/yeh803/cache/miniforge3/envs/preble/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/n/holystore01/LABS/mzitnik_lab/Lab/yeh803/cache/miniforge3/envs/preble/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/n/holystore01/LABS/mzitnik_lab/Lab/yeh803/cache/miniforge3/envs/preble/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/n/holystore01/LABS/mzitnik_lab/Lab/yeh803/cache/miniforge3/envs/preble/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/n/holystore01/LABS/mzitnik_lab/Lab/yeh803/cache/miniforge3/envs/preble/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/n/holystore01/LABS/mzitnik_lab/Lab/yeh803/cache/miniforge3/envs/preble/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/n/holystore01/LABS/mzitnik_lab/Lab/yeh803/cache/miniforge3/envs/preble/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/n/holystore01/LABS/mzitnik_lab/Lab/yeh803/cache/miniforge3/envs/preble/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/n/holystore01/LABS/mzitnik_lab/Lab/yeh803/cache/miniforge3/envs/preble/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.1654694080352783, 'eval_accuracy': 0.239453125, 'eval_f1': 0.2200286901177059, 'eval_precision': 0.22381679749779596, 'eval_recall': 0.239453125, 'eval_auroc': 0.8510000249295242, 'eval_mcc': 0.19968186894809048, 'eval_runtime': 4.8416, 'eval_samples_per_second': 2138.123, 'eval_steps_per_second': 2.272, 'epoch': 20.0}\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from copy import deepcopy\n",
    "# Model configuration\n",
    "config = {\n",
    "    \"base_model\": deepcopy(bert_model), \n",
    "    \"normalize_embeddings\": True, \n",
    "    \"adaptor_output_dim\": num_buckets, \n",
    "    \"adaptor_hidden_dims\": [256, 128], \n",
    "    \"adaptor_dropout\": 0.2, \n",
    "    \"adaptor_norm\": \"ln\", \n",
    "    \"adaptor_actn\": \"relu\", \n",
    "    \"adaptor_order\": \"nd\"\n",
    "}\n",
    "model = OutputLengthPredictor(**config)\n",
    "\n",
    "# Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",          # Output directory\n",
    "    evaluation_strategy=\"epoch\",    # Evaluate at the end of each epoch\n",
    "    per_device_train_batch_size=400,  # Batch size for training\n",
    "    per_device_eval_batch_size=800,   # Batch size for evaluation\n",
    "    warmup_steps=50,\n",
    "    num_train_epochs=50,             # Number of training epochs\n",
    "    logging_dir=\"./logs\",           # Directory for storing logs\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",          # Save checkpoint at the end of each epoch\n",
    "    fp16=False,                      # Enable mixed precision\n",
    "    dataloader_drop_last=True,      # Drop incomplete batches\n",
    "    report_to=\"none\",              # Disable Weights & Biases logging\n",
    "    load_best_model_at_end=True,     # Load the best model based on evaluation\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_small_ds_tokenized,\n",
    "    eval_dataset=val_ds_tokenized,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate on the test set\n",
    "results = trainer.evaluate(test_ds_tokenized)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "preble",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
